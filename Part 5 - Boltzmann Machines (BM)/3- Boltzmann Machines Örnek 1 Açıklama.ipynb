{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f521189f-fb9b-4478-aa3e-6e239bedae97",
   "metadata": {},
   "source": [
    "Harika—bu RBM örneğini hem **adım adım açıklayacağım**, hem de **daha sağlam (ufak düzeltmelerle)** çalışan bir örnek kodu paylaşacağım. Kod MovieLens **ml-100k** (u1.base / u1.test) üzerinde **Bernoulli–Bernoulli RBM** ile **beğeni (>=3)** / **beğenmedi (1–2)** ikilileştirilmiş (binary) tahmin yapıyor.\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Verdiğin kod ne yapıyor? (Adım adım)\n",
    "\n",
    "### (a) Veriyi okuma ve kullanıcı–film matrisine dönüştürme\n",
    "\n",
    "```python\n",
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter='\\t')\n",
    "test_set     = pd.read_csv('ml-100k/u1.test', delimiter='\\t')\n",
    "training_set = np.array(training_set, dtype='int')\n",
    "test_set     = np.array(test_set, dtype='int')\n",
    "\n",
    "nb_users  = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
    "```\n",
    "\n",
    "* `u1.base` ve `u1.test` “(user, movie, rating, timestamp)” satırlarıdır.\n",
    "* `nb_users`, `nb_movies` ile kullanıcı/film sayısını alıyor.\n",
    "\n",
    "```python\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies  = data[:,1][data[:,0] == id_users]\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)  # dolu olmayanlar = 0\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "\n",
    "training_set = convert(training_set)  # (nb_users, nb_movies)\n",
    "test_set     = convert(test_set)\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set     = torch.FloatTensor(test_set)\n",
    "```\n",
    "\n",
    "* **Yoğun** (dense) bir **kullanıcı × film** matrisi yapıyor: her satır bir kullanıcı, sütunlar filmler, değerler rating (0 = hiç oy yok).\n",
    "\n",
    "### (b) Binarizasyon ve “eksik”lerin işaretlenmesi\n",
    "\n",
    "```python\n",
    "training_set[training_set == 0]  = -1  # \"hiç oy yok\" -> -1 (mask işareti)\n",
    "training_set[training_set == 1]  = 0   # 1-2 -> 0 (beğenmedi)\n",
    "training_set[training_set == 2]  = 0\n",
    "training_set[training_set >= 3]  = 1   # 3-4-5 -> 1 (beğendi)\n",
    "\n",
    "test_set[test_set == 0]  = -1\n",
    "test_set[test_set == 1]  = 0\n",
    "test_set[test_set == 2]  = 0\n",
    "test_set[test_set >= 3]  = 1\n",
    "```\n",
    "\n",
    "* RBM görünür birimleri **Bernoulli** kabul ettiği için rating’leri **0/1**’e çeviriyor; **eksikler** `-1` ile işaretleniyor.\n",
    "\n",
    "### (c) RBM sınıfı (Bernoulli–Bernoulli)\n",
    "\n",
    "```python\n",
    "class RBM():\n",
    "    def __init__(self, nv, nh):\n",
    "        self.W = torch.randn(nh, nv)  # ağırlıklar (hidden x visible)\n",
    "        self.a = torch.randn(1, nh)   # gizli bias\n",
    "        self.b = torch.randn(1, nv)   # görünür bias\n",
    "```\n",
    "\n",
    "* `nv` = film sayısı (visible units), `nh` = gizli ünite sayısı.\n",
    "\n",
    "```python\n",
    "    def sample_h(self, x):\n",
    "        wx = torch.mm(x, self.W.t())\n",
    "        activation = wx + self.a.expand_as(wx)\n",
    "        p_h_given_v = torch.sigmoid(activation)\n",
    "        return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
    "\n",
    "    def sample_v(self, y):\n",
    "        wy = torch.mm(y, self.W)\n",
    "        activation = wy + self.b.expand_as(wy)\n",
    "        p_v_given_h = torch.sigmoid(activation)\n",
    "        return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
    "```\n",
    "\n",
    "* **Koşullu** olasılıklar: $p(h|v)$ ve $p(v|h)$ sigmoid; ve Bernoulli örnekleme ile ikili değer üretiliyor.\n",
    "\n",
    "```python\n",
    "    def train(self, v0, vk, ph0, phk):\n",
    "        self.W += torch.mm(v0.t(), ph0).t() - torch.mm(vk.t(), phk).t()\n",
    "        self.b += torch.sum((v0 - vk), 0)\n",
    "        self.a += torch.sum((ph0 - phk), 0)\n",
    "```\n",
    "\n",
    "* **Contrastive Divergence** güncellemesi:\n",
    "  **pozitif faz** $v_0, p(h|v_0)$ – **negatif faz** $v_k, p(h|v_k)$.\n",
    "  (Not: burada **öğrenme oranı yok**, **batch’e bölünmüyor**; pratikte LR ve ‘/batch\\_size’ eklemek iyidir.)\n",
    "\n",
    "### (d) Eğitim döngüsü (CD-10, mini-batch=100 kullanıcı)\n",
    "\n",
    "```python\n",
    "nv = len(training_set[0]); nh = 100; batch_size = 100\n",
    "rbm = RBM(nv, nh)\n",
    "nb_epoch = 10\n",
    "\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0; s = 0.\n",
    "    for id_user in range(0, nb_users - batch_size, batch_size):\n",
    "        vk = training_set[id_user:id_user+batch_size]  # negatif zincire girecek kopya\n",
    "        v0 = training_set[id_user:id_user+batch_size]  # orijinal batch\n",
    "        ph0,_ = rbm.sample_h(v0)\n",
    "        for k in range(10):   # CD-10\n",
    "            _,hk = rbm.sample_h(vk)\n",
    "            _,vk = rbm.sample_v(hk)\n",
    "            vk[v0<0] = v0[v0<0]  # eksik olanları (-1) olduğu gibi bırak\n",
    "        phk,_ = rbm.sample_h(vk)\n",
    "        rbm.train(v0, vk, ph0, phk)\n",
    "        train_loss += torch.mean(torch.abs(v0[v0>=0] - vk[v0>=0]))\n",
    "        s += 1.\n",
    "    print('epoch:', epoch, 'loss:', train_loss/s)\n",
    "```\n",
    "\n",
    "* **CD-10** zinciri: $v_0 \\to h_0 \\to v_1 \\to h_1 \\dots \\to v_{10}$.\n",
    "* `vk[v0<0]=v0[v0<0]` ile **eksik** değerleri zincir boyunca **sabit tutuyor** (rekonstrüksiyon ve kayıp bu noktalarda hesaplanmıyor).\n",
    "* Kayıp: **bilinen** girdilerde $|v_0 - v_k|$ (MAE).\n",
    "\n",
    "### (e) Test\n",
    "\n",
    "```python\n",
    "test_loss = 0; s = 0.\n",
    "for id_user in range(nb_users):\n",
    "    v  = training_set[id_user:id_user+1]\n",
    "    vt = test_set[id_user:id_user+1]\n",
    "    if len(vt[vt>=0]) > 0:\n",
    "        _,h = rbm.sample_h(v)\n",
    "        _,v = rbm.sample_v(h)  # rekonstrüksiyon\n",
    "        test_loss += torch.mean(torch.abs(vt[vt>=0] - v[vt>=0]))\n",
    "        s += 1.\n",
    "print('test loss:', test_loss/s)\n",
    "```\n",
    "\n",
    "* Her kullanıcı için test setindeki **bilinen** filmlerde MAE bakıyor.\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Önemli notlar / iyileştirmeler\n",
    "\n",
    "1. **Öğrenme oranı (lr) ve batch bölme**\n",
    "   Kod, ağırlıkları **ölçeksiz** güncelliyor. Pratikte:\n",
    "\n",
    "   $$\n",
    "   W \\leftarrow W + \\eta \\frac{(v_0^\\top p(h|v_0) - v_k^\\top p(h|v_k))^\\top}{\\text{batch\\_size}}\n",
    "   $$\n",
    "\n",
    "   ve bias’ları da `η/batch_size` ile güncellemek daha stabil.\n",
    "\n",
    "2. **Eksikleri -1 ile beslemek**\n",
    "   `sample_h(v0)` çağrısında `v0` içinde **-1**’ler var; bu, Bernoulli RBM varsayımıyla **tutarlı değil**.\n",
    "   Daha doğru yaklaşım: **mask** tut, **pozitif faz** girişini `v0_missing→0` yaparak ver (veya inputu 0/1’e çevirip eksikleri 0 say), kaybı yalnız **bilinen**lerde hesapla.\n",
    "\n",
    "3. **Başlatma ve düzenleme (regularization)**\n",
    "\n",
    "   * Ağırlıkları **Xavier/He** gibi küçük ölçekle başlat.\n",
    "   * **Momentum / weight decay** eklemek işe yarar.\n",
    "\n",
    "4. **Mini-batch & shuffle**\n",
    "   Kullanıcıları her epoch **karıştır** (shuffle). DataLoader ile daha temiz.\n",
    "\n",
    "5. **Metrikler**\n",
    "   İkili yapıdaysan testte **AUC / Precision\\@K / Recall\\@K** gibi metrikler daha anlamlı olabilir.\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Düzeltilmiş ve açıklamalı örnek kod\n",
    "\n",
    "Aşağıdaki sürüm, aynı fikri korur ama:\n",
    "\n",
    "* **LR** ve **/batch\\_size** ekler,\n",
    "* **mask** kullanır: eksikleri modele **0** verip, **pozitif faz** ve kaybı yalnız **bilinen**lerde hesaplar,\n",
    "* **shuffle** ile DataLoader kullanır,\n",
    "* `cd_k` parametreleştirir (CD-1 çoğu zaman yeterli).\n",
    "\n",
    "> `pip install torch pandas` (MovieLens dosyaları aynı konumda olmalı)\n",
    "\n",
    "```python\n",
    "# rbm_movielens_clean.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 1) Veri: u1.base / u1.test -> kullanıcı×film matrisi\n",
    "def load_ml100k(base_path=\"ml-100k\"):\n",
    "    train = pd.read_csv(f\"{base_path}/u1.base\", delimiter=\"\\t\", header=None, names=[\"u\",\"i\",\"r\",\"t\"]).values\n",
    "    test  = pd.read_csv(f\"{base_path}/u1.test\", delimiter=\"\\t\", header=None, names=[\"u\",\"i\",\"r\",\"t\"]).values\n",
    "    nb_users  = int(max(train[:,0].max(), test[:,0].max()))\n",
    "    nb_items  = int(max(train[:,1].max(), test[:,1].max()))\n",
    "\n",
    "    def convert(data):\n",
    "        X = np.zeros((nb_users, nb_items), dtype=np.float32)\n",
    "        for u in range(1, nb_users+1):\n",
    "            movies  = data[:,1][data[:,0]==u]\n",
    "            ratings = data[:,2][data[:,0]==u]\n",
    "            X[u-1, movies-1] = ratings\n",
    "        return X\n",
    "\n",
    "    Xtr = convert(train)  # rating: 0 (none) or 1..5\n",
    "    Xte = convert(test)\n",
    "    return Xtr, Xte\n",
    "\n",
    "Xtr, Xte = load_ml100k()\n",
    "\n",
    "# 2) Binarize: 1-2 -> 0, 3-4-5 -> 1; missing(0) -> mask\n",
    "def binarize(X):\n",
    "    X_bin = X.copy()\n",
    "    mask  = (X_bin > 0)  # known ratings\n",
    "    X_bin[X_bin <= 2] = 0\n",
    "    X_bin[X_bin >= 3] = 1\n",
    "    # Eksikleri 0 olarak bırakıyoruz; mask ile ayrıştıracağız\n",
    "    return X_bin.astype(np.float32), mask.astype(np.float32)\n",
    "\n",
    "Vtr, Mtr = binarize(Xtr)\n",
    "Vte, Mte = binarize(Xte)\n",
    "\n",
    "Vtr_t = torch.from_numpy(Vtr)  # (U, I)\n",
    "Mtr_t = torch.from_numpy(Mtr)  # (U, I)\n",
    "Vte_t = torch.from_numpy(Vte)\n",
    "Mte_t = torch.from_numpy(Mte)\n",
    "\n",
    "# 3) RBM sınıfı (Bernoulli-Bernoulli)\n",
    "class RBM:\n",
    "    def __init__(self, nv, nh, lr=0.01, cd_k=1, device=\"cpu\"):\n",
    "        self.nv, self.nh = nv, nh\n",
    "        self.lr   = lr\n",
    "        self.cd_k = cd_k\n",
    "        self.W = torch.randn(nh, nv, device=device) * 0.01\n",
    "        self.a = torch.zeros(1, nh, device=device)   # hidden bias\n",
    "        self.b = torch.zeros(1, nv, device=device)   # visible bias\n",
    "        self.device = device\n",
    "\n",
    "    def sample_h(self, v):\n",
    "        # v: (B, nv)\n",
    "        wx = v @ self.W.t() + self.a.expand_as(v @ self.W.t())\n",
    "        p_h = torch.sigmoid(wx)\n",
    "        h   = torch.bernoulli(p_h)\n",
    "        return p_h, h\n",
    "\n",
    "    def sample_v(self, h):\n",
    "        wy = h @ self.W + self.b.expand_as(h @ self.W)\n",
    "        p_v = torch.sigmoid(wy)\n",
    "        v   = torch.bernoulli(p_v)\n",
    "        return p_v, v\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train_step(self, v0, mask):\n",
    "        \"\"\"\n",
    "        v0: (B, nv)  -> 0/1, missingler 0 (mask=0)\n",
    "        mask: (B, nv) -> 1: known, 0: missing\n",
    "        CD-k: known pozisyonları 'clamp' etmek için mask kullanabiliriz.\n",
    "        \"\"\"\n",
    "        B = v0.size(0)\n",
    "\n",
    "        # Pozitif faz\n",
    "        ph0, h0 = self.sample_h(v0)\n",
    "\n",
    "        # Negatif faz (CD-k)\n",
    "        vk = v0.clone()\n",
    "        for _ in range(self.cd_k):\n",
    "            ph, h = self.sample_h(vk)\n",
    "            pv, v = self.sample_v(h)\n",
    "            # İsteğe bağlı: known pozisyonları orijinalde sabit tut\n",
    "            vk = vk * (1 - mask) + v * mask  # clamp knowns to model sample? (alternatif: vk[mask==1]=v0[mask==1])\n",
    "\n",
    "        phk, _ = self.sample_h(vk)\n",
    "\n",
    "        # Ağırlık güncelleme (lr ve batch bölme ile)\n",
    "        self.W += self.lr * ((v0.t() @ ph0) - (vk.t() @ phk)).t() / B\n",
    "        self.b += self.lr * torch.sum((v0 - vk), dim=0, keepdim=True) / B\n",
    "        self.a += self.lr * torch.sum((ph0 - phk), dim=0, keepdim=True) / B\n",
    "\n",
    "        # Kayıp: yalnız knownlarda MAE\n",
    "        loss = torch.mean(torch.abs((v0 - vk)[mask.bool()]))\n",
    "        return loss.item()\n",
    "\n",
    "# 4) Eğitim\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "nv = Vtr_t.size(1)\n",
    "rbm = RBM(nv=nv, nh=100, lr=0.05, cd_k=1, device=device)\n",
    "\n",
    "dataset = TensorDataset(Vtr_t.to(device), Mtr_t.to(device))\n",
    "loader  = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    for v0, mask in loader:\n",
    "        loss = rbm.train_step(v0, mask)\n",
    "        losses.append(loss)\n",
    "    print(f\"Epoch {epoch+1:02d} | train MAE(on known): {np.mean(losses):.4f}\")\n",
    "\n",
    "# 5) Test: rekonstrüksiyon hatası (sadece test setindeki knownlarda)\n",
    "@torch.no_grad()\n",
    "def eval_mae(rbm, V, M):\n",
    "    V = V.to(rbm.device); M = M.to(rbm.device)\n",
    "    ph, h = rbm.sample_h(V)\n",
    "    pv, v = rbm.sample_v(h)\n",
    "    mae = torch.mean(torch.abs((V - v)[M.bool()])).item()\n",
    "    return mae\n",
    "\n",
    "test_mae = eval_mae(rbm, Vte_t, Mte_t)\n",
    "print(f\"Test MAE (known entries): {test_mae:.4f}\")\n",
    "```\n",
    "\n",
    "**Notlar (kod):**\n",
    "\n",
    "* **Mask** ile eksikleri net biçimde ayırdık; **girişleri 0** olarak verip kaybı **yalnız bilinenlerde** ölçüyoruz.\n",
    "* `train_step`’te **öğrenme oranı** ve **/batch\\_size** var → daha kararlı.\n",
    "* `cd_k=1` çoğu zaman yeter; istersen 5–10 deneyebilirsin (daha yavaş).\n",
    "* İstersen `vk = v0 * mask + v * (1 - mask)` şeklinde **knownları tamamen clamp** edip sadece eksikleri üretmek de bir tercih (demo amaçlı yukarıdaki harman yöntemi bırakıldı).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Kapanış – özet\n",
    "\n",
    "* RBM, **beğeni/ beğenmeme** gibi **ikili** görünür değişkenlerle iyi çalışır → MovieLens’i bu yüzden **binary** yaptık.\n",
    "* Eğitimde asıl fikir: **pozitif faz** (veriyle hizalan) – **negatif faz** (modelin kendi örnekleriyle uzaklaş).\n",
    "* Pratikte **lr/batch**, **mask**, **shuffle**, **cd\\_k**, **başlatma** ve **regularization** modeli ciddi biçimde stabilize eder.\n",
    "\n",
    "İstersen bir sonraki adımda **Gaussian–Bernoulli RBM** (sürekli görünürler için) veya **öneri metrikleri** (Precision\\@K/Recall\\@K, AUC) ekleyebiliriz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15497aa-458a-46ea-a8aa-612ce547d5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
