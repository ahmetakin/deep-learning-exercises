{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d173e30-96c3-471f-a17e-4ead9e73e1cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Yapay Sinir Ağı (ANN) Nedir?\n",
    "\n",
    "* **İlham kaynağı:** İnsan beynindeki **nöronlar**. Beyinde her nöron binlerce başka nöronla bağlantılıdır.\n",
    "* **Amaç:** Çok karmaşık ilişkileri ve doğrusal olmayan (non-linear) yapıları öğrenebilmek.\n",
    "* **Kullanım alanı:** Görüntü tanıma, doğal dil işleme, tavsiye sistemleri, anomali tespiti, tahmin modelleri.\n",
    "* **Avantajı:** Kendi kendine “özellik çıkarımı” yapabilir. Yani klasik makine öğrenmesinde manuel feature engineering gerekirken ANN bunu otomatik öğrenebilir.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ANN Yapısı\n",
    "\n",
    "Bir ANN tipik olarak 3 katmandan oluşur:\n",
    "\n",
    "1. **Input Layer (Girdi Katmanı):** Verilerin geldiği yer. Örn: müşteri yaşı, maaşı, kredi skoru.\n",
    "2. **Hidden Layers (Gizli Katmanlar):** Asıl öğrenmenin olduğu kısım. Burada nöronlar girişleri ağırlıklarla çarpar, bias ekler, aktivasyon fonksiyonu uygular. Katman sayısı ve nöron sayısı modeli güçlendirir.\n",
    "3. **Output Layer (Çıktı Katmanı):** Modelin tahmini. Örn: kredi onay (evet/hayır), resimde kedi mi köpek mi, hisse fiyatı.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Nöronun Matematiği\n",
    "\n",
    "Bir nöronun yaptığı işlem aslında şudur:\n",
    "\n",
    "$$\n",
    "z = w_1x_1 + w_2x_2 + ... + w_nx_n + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "a = f(z)\n",
    "$$\n",
    "\n",
    "* $x_i$ → girişler\n",
    "* $w_i$ → ağırlıklar (modelin öğrenmeye çalıştığı parametreler)\n",
    "* $b$ → bias (dengelemek için eklenen parametre)\n",
    "* $f$ → aktivasyon fonksiyonu (Sigmoid, ReLU, tanh vs.)\n",
    "* $a$ → nöronun çıktısı\n",
    "\n",
    "👉 Bu kadar basit: girişleri al, ağırlıklarla çarp, bias ekle, aktivasyon fonksiyonundan geçir.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Aktivasyon Fonksiyonları\n",
    "\n",
    "Beyindeki sinyaller gibi, ANN’de de doğrusal olmayan dönüşümlere ihtiyacımız var:\n",
    "\n",
    "* **Sigmoid:** Çıktıyı 0–1 arası sıkıştırır. İkili sınıflandırmada kullanılır.\n",
    "* **Tanh:** Çıktıyı -1 ile 1 arası sıkıştırır.\n",
    "* **ReLU (Rectified Linear Unit):** Negatifleri 0 yapar, pozitifleri aynen geçirir. Çok popülerdir çünkü hızlı öğrenir.\n",
    "* **Softmax:** Çok sınıflı sınıflandırmada olasılık dağılımı verir.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Öğrenme Mekanizması (Training)\n",
    "\n",
    "ANN’nin öğrenmesi 3 ana adımdan oluşur:\n",
    "\n",
    "1. **Forward Propagation (İleri Yayılım):**\n",
    "   Girdi → gizli katmanlar → çıktı.\n",
    "   Tahmin yapılır.\n",
    "\n",
    "2. **Cost Function (Maliyet Fonksiyonu):**\n",
    "   Modelin ne kadar hata yaptığını ölçer.\n",
    "\n",
    "   * Regresyonda genelde **MSE (Mean Squared Error)**\n",
    "   * Sınıflandırmada **Cross-Entropy Loss** kullanılır.\n",
    "\n",
    "3. **Backpropagation (Geri Yayılım):**\n",
    "   Hata geriye doğru yayılır. Her ağırlığın hatadaki katkısı hesaplanır ve **gradient descent** ile güncellenir.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Gradient Descent\n",
    "\n",
    "Gradient descent, ANN’nin öğrenme motorudur:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "* $J$ → cost function\n",
    "* $\\alpha$ → öğrenme oranı (learning rate)\n",
    "* $\\frac{\\partial J}{\\partial w}$ → türev (hata yüzünden ağırlığın ne kadar değişmesi gerektiğini söyler)\n",
    "\n",
    "👉 Böylece her adımda hata biraz daha azalır, model öğrenir.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Özet (Senin İçin Sezgisel Çerçeve)\n",
    "\n",
    "* ANN = çok sayıda matematiksel fonksiyonun birbirine bağlanmış hali.\n",
    "* Her nöron basit bir işlem yapar ama binlercesi birleştiğinde **çok güçlü** olur.\n",
    "* Öğrenme: **Forward → Hata Hesabı → Backpropagation → Gradient Descent** döngüsü.\n",
    "* Aktivasyon fonksiyonları modeli doğrusal olmayan problemlerde güçlü yapar.\n",
    "* Parametreler (weights, bias) eğitim sürecinde optimize edilir.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Stokastik Gradient Descent ile Yapay Sinir Ağının Eğitilmesi\n",
    "\n",
    "**Adım 1:** Ağırlıkları, sıfıra çok yakın küçük rastgele sayılarla başlat (ama 0 olmasın).\n",
    "\n",
    "**Adım 2:** Veri setindeki ilk gözlemi (örneği) giriş katmanına ver. Her özellik bir giriş düğümüne karşılık gelir.\n",
    "\n",
    "**Adım 3:** **İleri Yayılım (Forward Propagation):** Soldan sağa doğru nöronlar aktive edilir. Her nöronun çıktısı ağırlıklarla sınırlıdır. Aktivasyonlar katmanlar boyunca iletilir ve tahmini sonuç $\\hat{y}$ elde edilir.\n",
    "\n",
    "**Adım 4:** Tahmin edilen sonuç ile gerçek sonucu karşılaştır. Ortaya çıkan hatayı ölç.\n",
    "\n",
    "**Adım 5:** **Geri Yayılım (Backpropagation):** Sağdan sola doğru hata geri yayılır. Her ağırlığın hatadaki sorumluluğuna göre güncelleme yapılır. Öğrenme oranı (learning rate), ağırlıkların ne kadar güncelleneceğini belirler.\n",
    "\n",
    "**Adım 6:** Adım 1–5’i tekrarla ve her gözlemden sonra ağırlıkları güncelle (Stokastik/Çevrim içi öğrenme). Alternatif olarak, bir grup (batch) gözlemden sonra güncelleme yapılabilir (Batch Learning).\n",
    "\n",
    "**Adım 7:** Tüm eğitim verisi ağa bir kez geçirildiğinde bu bir **epoch** olur. Daha iyi sonuç için birden fazla epoch yapılır.\n",
    "\n",
    "---\n",
    "\n",
    "👉 Özetle: ANN eğitimi sürekli olarak **ileri yayılım → hata hesaplama → geri yayılım → ağırlık güncelleme** döngüsüyle yapılır. Bu süreç epoch’lar boyunca tekrarlanır ve model hatayı minimize ederek öğrenir.\n",
    "\n",
    "---\n",
    "\n",
    "İstersen buradan sonra ben sana **SGD (Stochastic Gradient Descent) ile Batch Gradient Descent farklarını** da açıklayayım mı? Çünkü bu kısım genelde kafaları karıştırıyor.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
