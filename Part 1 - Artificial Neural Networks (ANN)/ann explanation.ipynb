{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d173e30-96c3-471f-a17e-4ead9e73e1cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Yapay Sinir AÄŸÄ± (ANN) Nedir?\n",
    "\n",
    "* **Ä°lham kaynaÄŸÄ±:** Ä°nsan beynindeki **nÃ¶ronlar**. Beyinde her nÃ¶ron binlerce baÅŸka nÃ¶ronla baÄŸlantÄ±lÄ±dÄ±r.\n",
    "* **AmaÃ§:** Ã‡ok karmaÅŸÄ±k iliÅŸkileri ve doÄŸrusal olmayan (non-linear) yapÄ±larÄ± Ã¶ÄŸrenebilmek.\n",
    "* **KullanÄ±m alanÄ±:** GÃ¶rÃ¼ntÃ¼ tanÄ±ma, doÄŸal dil iÅŸleme, tavsiye sistemleri, anomali tespiti, tahmin modelleri.\n",
    "* **AvantajÄ±:** Kendi kendine â€œÃ¶zellik Ã§Ä±karÄ±mÄ±â€ yapabilir. Yani klasik makine Ã¶ÄŸrenmesinde manuel feature engineering gerekirken ANN bunu otomatik Ã¶ÄŸrenebilir.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ANN YapÄ±sÄ±\n",
    "\n",
    "Bir ANN tipik olarak 3 katmandan oluÅŸur:\n",
    "\n",
    "1. **Input Layer (Girdi KatmanÄ±):** Verilerin geldiÄŸi yer. Ã–rn: mÃ¼ÅŸteri yaÅŸÄ±, maaÅŸÄ±, kredi skoru.\n",
    "2. **Hidden Layers (Gizli Katmanlar):** AsÄ±l Ã¶ÄŸrenmenin olduÄŸu kÄ±sÄ±m. Burada nÃ¶ronlar giriÅŸleri aÄŸÄ±rlÄ±klarla Ã§arpar, bias ekler, aktivasyon fonksiyonu uygular. Katman sayÄ±sÄ± ve nÃ¶ron sayÄ±sÄ± modeli gÃ¼Ã§lendirir.\n",
    "3. **Output Layer (Ã‡Ä±ktÄ± KatmanÄ±):** Modelin tahmini. Ã–rn: kredi onay (evet/hayÄ±r), resimde kedi mi kÃ¶pek mi, hisse fiyatÄ±.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. NÃ¶ronun MatematiÄŸi\n",
    "\n",
    "Bir nÃ¶ronun yaptÄ±ÄŸÄ± iÅŸlem aslÄ±nda ÅŸudur:\n",
    "\n",
    "$$\n",
    "z = w_1x_1 + w_2x_2 + ... + w_nx_n + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "a = f(z)\n",
    "$$\n",
    "\n",
    "* $x_i$ â†’ giriÅŸler\n",
    "* $w_i$ â†’ aÄŸÄ±rlÄ±klar (modelin Ã¶ÄŸrenmeye Ã§alÄ±ÅŸtÄ±ÄŸÄ± parametreler)\n",
    "* $b$ â†’ bias (dengelemek iÃ§in eklenen parametre)\n",
    "* $f$ â†’ aktivasyon fonksiyonu (Sigmoid, ReLU, tanh vs.)\n",
    "* $a$ â†’ nÃ¶ronun Ã§Ä±ktÄ±sÄ±\n",
    "\n",
    "ğŸ‘‰ Bu kadar basit: giriÅŸleri al, aÄŸÄ±rlÄ±klarla Ã§arp, bias ekle, aktivasyon fonksiyonundan geÃ§ir.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Aktivasyon FonksiyonlarÄ±\n",
    "\n",
    "Beyindeki sinyaller gibi, ANNâ€™de de doÄŸrusal olmayan dÃ¶nÃ¼ÅŸÃ¼mlere ihtiyacÄ±mÄ±z var:\n",
    "\n",
    "* **Sigmoid:** Ã‡Ä±ktÄ±yÄ± 0â€“1 arasÄ± sÄ±kÄ±ÅŸtÄ±rÄ±r. Ä°kili sÄ±nÄ±flandÄ±rmada kullanÄ±lÄ±r.\n",
    "* **Tanh:** Ã‡Ä±ktÄ±yÄ± -1 ile 1 arasÄ± sÄ±kÄ±ÅŸtÄ±rÄ±r.\n",
    "* **ReLU (Rectified Linear Unit):** Negatifleri 0 yapar, pozitifleri aynen geÃ§irir. Ã‡ok popÃ¼lerdir Ã§Ã¼nkÃ¼ hÄ±zlÄ± Ã¶ÄŸrenir.\n",
    "* **Softmax:** Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rmada olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± verir.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Ã–ÄŸrenme MekanizmasÄ± (Training)\n",
    "\n",
    "ANNâ€™nin Ã¶ÄŸrenmesi 3 ana adÄ±mdan oluÅŸur:\n",
    "\n",
    "1. **Forward Propagation (Ä°leri YayÄ±lÄ±m):**\n",
    "   Girdi â†’ gizli katmanlar â†’ Ã§Ä±ktÄ±.\n",
    "   Tahmin yapÄ±lÄ±r.\n",
    "\n",
    "2. **Cost Function (Maliyet Fonksiyonu):**\n",
    "   Modelin ne kadar hata yaptÄ±ÄŸÄ±nÄ± Ã¶lÃ§er.\n",
    "\n",
    "   * Regresyonda genelde **MSE (Mean Squared Error)**\n",
    "   * SÄ±nÄ±flandÄ±rmada **Cross-Entropy Loss** kullanÄ±lÄ±r.\n",
    "\n",
    "3. **Backpropagation (Geri YayÄ±lÄ±m):**\n",
    "   Hata geriye doÄŸru yayÄ±lÄ±r. Her aÄŸÄ±rlÄ±ÄŸÄ±n hatadaki katkÄ±sÄ± hesaplanÄ±r ve **gradient descent** ile gÃ¼ncellenir.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Gradient Descent\n",
    "\n",
    "Gradient descent, ANNâ€™nin Ã¶ÄŸrenme motorudur:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "* $J$ â†’ cost function\n",
    "* $\\alpha$ â†’ Ã¶ÄŸrenme oranÄ± (learning rate)\n",
    "* $\\frac{\\partial J}{\\partial w}$ â†’ tÃ¼rev (hata yÃ¼zÃ¼nden aÄŸÄ±rlÄ±ÄŸÄ±n ne kadar deÄŸiÅŸmesi gerektiÄŸini sÃ¶yler)\n",
    "\n",
    "ğŸ‘‰ BÃ¶ylece her adÄ±mda hata biraz daha azalÄ±r, model Ã¶ÄŸrenir.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Ã–zet (Senin Ä°Ã§in Sezgisel Ã‡erÃ§eve)\n",
    "\n",
    "* ANN = Ã§ok sayÄ±da matematiksel fonksiyonun birbirine baÄŸlanmÄ±ÅŸ hali.\n",
    "* Her nÃ¶ron basit bir iÅŸlem yapar ama binlercesi birleÅŸtiÄŸinde **Ã§ok gÃ¼Ã§lÃ¼** olur.\n",
    "* Ã–ÄŸrenme: **Forward â†’ Hata HesabÄ± â†’ Backpropagation â†’ Gradient Descent** dÃ¶ngÃ¼sÃ¼.\n",
    "* Aktivasyon fonksiyonlarÄ± modeli doÄŸrusal olmayan problemlerde gÃ¼Ã§lÃ¼ yapar.\n",
    "* Parametreler (weights, bias) eÄŸitim sÃ¼recinde optimize edilir.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Stokastik Gradient Descent ile Yapay Sinir AÄŸÄ±nÄ±n EÄŸitilmesi\n",
    "\n",
    "**AdÄ±m 1:** AÄŸÄ±rlÄ±klarÄ±, sÄ±fÄ±ra Ã§ok yakÄ±n kÃ¼Ã§Ã¼k rastgele sayÄ±larla baÅŸlat (ama 0 olmasÄ±n).\n",
    "\n",
    "**AdÄ±m 2:** Veri setindeki ilk gÃ¶zlemi (Ã¶rneÄŸi) giriÅŸ katmanÄ±na ver. Her Ã¶zellik bir giriÅŸ dÃ¼ÄŸÃ¼mÃ¼ne karÅŸÄ±lÄ±k gelir.\n",
    "\n",
    "**AdÄ±m 3:** **Ä°leri YayÄ±lÄ±m (Forward Propagation):** Soldan saÄŸa doÄŸru nÃ¶ronlar aktive edilir. Her nÃ¶ronun Ã§Ä±ktÄ±sÄ± aÄŸÄ±rlÄ±klarla sÄ±nÄ±rlÄ±dÄ±r. Aktivasyonlar katmanlar boyunca iletilir ve tahmini sonuÃ§ $\\hat{y}$ elde edilir.\n",
    "\n",
    "**AdÄ±m 4:** Tahmin edilen sonuÃ§ ile gerÃ§ek sonucu karÅŸÄ±laÅŸtÄ±r. Ortaya Ã§Ä±kan hatayÄ± Ã¶lÃ§.\n",
    "\n",
    "**AdÄ±m 5:** **Geri YayÄ±lÄ±m (Backpropagation):** SaÄŸdan sola doÄŸru hata geri yayÄ±lÄ±r. Her aÄŸÄ±rlÄ±ÄŸÄ±n hatadaki sorumluluÄŸuna gÃ¶re gÃ¼ncelleme yapÄ±lÄ±r. Ã–ÄŸrenme oranÄ± (learning rate), aÄŸÄ±rlÄ±klarÄ±n ne kadar gÃ¼ncelleneceÄŸini belirler.\n",
    "\n",
    "**AdÄ±m 6:** AdÄ±m 1â€“5â€™i tekrarla ve her gÃ¶zlemden sonra aÄŸÄ±rlÄ±klarÄ± gÃ¼ncelle (Stokastik/Ã‡evrim iÃ§i Ã¶ÄŸrenme). Alternatif olarak, bir grup (batch) gÃ¶zlemden sonra gÃ¼ncelleme yapÄ±labilir (Batch Learning).\n",
    "\n",
    "**AdÄ±m 7:** TÃ¼m eÄŸitim verisi aÄŸa bir kez geÃ§irildiÄŸinde bu bir **epoch** olur. Daha iyi sonuÃ§ iÃ§in birden fazla epoch yapÄ±lÄ±r.\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ Ã–zetle: ANN eÄŸitimi sÃ¼rekli olarak **ileri yayÄ±lÄ±m â†’ hata hesaplama â†’ geri yayÄ±lÄ±m â†’ aÄŸÄ±rlÄ±k gÃ¼ncelleme** dÃ¶ngÃ¼sÃ¼yle yapÄ±lÄ±r. Bu sÃ¼reÃ§ epochâ€™lar boyunca tekrarlanÄ±r ve model hatayÄ± minimize ederek Ã¶ÄŸrenir.\n",
    "\n",
    "---\n",
    "\n",
    "Ä°stersen buradan sonra ben sana **SGD (Stochastic Gradient Descent) ile Batch Gradient Descent farklarÄ±nÄ±** da aÃ§Ä±klayayÄ±m mÄ±? Ã‡Ã¼nkÃ¼ bu kÄ±sÄ±m genelde kafalarÄ± karÄ±ÅŸtÄ±rÄ±yor.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
