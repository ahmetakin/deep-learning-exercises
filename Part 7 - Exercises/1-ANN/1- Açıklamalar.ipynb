{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146d50cb-e8a9-46ba-97e0-6757fa84c58b",
   "metadata": {},
   "source": [
    "Süper bir kurgu yapmışsın. Aşağıda kodundaki her adımı, “neden böyle?” sorularını tek tek açıklıyorum; ardından verdiğin örnek satırlar ile **tahmin alma**, **modeli kaydetme/yükleme** ve **yeni veriyle devam eğitimi (fine-tune)** için tam kod veriyorum.\n",
    "\n",
    "---\n",
    "\n",
    "# Adım Adım Açıklamalar\n",
    "\n",
    "## 1) `y` için `.values` kullandık, `X` için kullanmadık — neden?\n",
    "\n",
    "* `y = dataset[\"Injury_Next_Season\"].values` ile **hedefi** `numpy` dizisine çeviriyorsun. Sklearn/Keras ikisiyle de uyumlu.\n",
    "* `X`’i **DataFrame** olarak tutmak önemli; çünkü **`ColumnTransformer` kolon adlarına göre** (isimle) işlem yapıyor. Eğer başta `X.values` yapsaydın, kolon adları kaybolur, seçtiğin sayısal/kategorik sütun listeleri boşa düşer.\n",
    "* Özet: **y = array**, **X = DataFrame** kalmalı.\n",
    "\n",
    "## 2) Neden iki aşamalı bölme: `X_train, X_temp, ...` ve sonra `X_valid, X_test, ...`?\n",
    "\n",
    "* Eğitim/Doğrulama/Test olarak **3’e bölmek** istiyoruz.\n",
    "* Sklearn’de tek fonksiyonla 3’lü split yok, bu yüzden:\n",
    "\n",
    "  1. Önce Train vs Temp (Train %70 – Temp %30)\n",
    "  2. Sonra Temp’i Valid vs Test (%15 – %15) diye ikiye bölüyoruz.\n",
    "* `stratify=y` sınıf dağılımını her bölmede korur (denge bozulmaz).\n",
    "\n",
    "## 3) Neden `fit_transform(train)` ama sadece `transform(valid/test)`?\n",
    "\n",
    "* **Veri sızıntısı (data leakage)** olmaması için **scaler/ohe/imputer** gibi istatistikleri **sadece train** üzerinde öğrenmeliyiz.\n",
    "* `preprocess.fit_transform(X_train)`: imputer medyanları, OHE kategori listesi, scaler ortalama-std **train’de fit edilir** ve train’e uygulanır.\n",
    "* `preprocess.transform(X_valid/X_test)`: valid/test üzerinde **fit edilmez**, sadece **aynı dönüşüm** uygulanır.\n",
    "* `feature_dim = X_train_p.shape[1]`: OHE’den sonra **özellik sayısı artabilir**; bu nedenle gerçek giriş boyutunu **pipeline çıktısından** ölçmek gerekir. ANN giriş katmanını buna göre kuruyoruz.\n",
    "\n",
    "## 4) `class_weight` neden hesaplandı?\n",
    "\n",
    "* Sınıflar **tam dengeli** bile olsa, küçük veri setlerinde veya loss yüzünden **minör dengesizlik etkilerini** telafi etmek iyi sonuç verir.\n",
    "* `compute_class_weight(..., y=y_train)` sadece **train** dağılımına göre **\\[sınıf -> ağırlık]** hesaplar; `model.fit(..., class_weight=...)` ile loss tarafında pozitif/negatif hataları eşitler.\n",
    "* Veri dengeli ise ağırlıklar genelde 1’e yakın çıkar; dengesiz olsa bile **küçük eforla büyük fark** yaratabilir.\n",
    "\n",
    "## 5) `build_model` ve “Sequential yapmadık mı?” sorusu\n",
    "\n",
    "* Aslında **`keras.Sequential`** kullandın. Yani “Sequential yapmadık” değil; **tam olarak Sequential yaptın** 🙂\n",
    "* Alternatif olarak **Functional API** de kullanılabilirdi (çoklu girdi, yan kol, embedding vs. gerektiğinde).\n",
    "* Buradaki katmanlar: `Dense(64, relu) -> Dropout -> Dense(32, relu) -> Dropout/2 -> Dense(1, sigmoid)`.\n",
    "\n",
    "  * **ReLU**: derin ağlarda iyi çalışır.\n",
    "  * **Dropout**: aşırı öğrenmeyi (overfit) azaltır.\n",
    "  * **Sigmoid**: ikili sınıflandırma çıkışı (0–1 olasılık).\n",
    "\n",
    "## 6) `EarlyStopping` ve `ReduceLROnPlateau` (plateau)\n",
    "\n",
    "* **EarlyStopping**: Val setindeki bir metriği izler (sen **`val_auc`** seçmişsin). Bir süre **iyileşme olmazsa** eğitimi durdurur ve **en iyi ağırlıkları geri yükler** (`restore_best_weights=True`). Overfit’i önler, zamanı kurtarır.\n",
    "* **ReduceLROnPlateau**: Val loss iyileşmeyi bırakınca **öğrenme oranını düşürür** (`factor=0.5`). Büyük LR ile “takılıp” kalmayı önler, daha küçük adımlarla iyileşme şansı verir.\n",
    "\n",
    "## 7) `history` nedir?\n",
    "\n",
    "* `model.fit(...)` **her epoch’taki loss/metric** değerlerini bir sözlükte toplar (`history.history`).\n",
    "* Bunu `pd.DataFrame`’e çevirip grafiklemek, **overfit/underfit** tanısını kolaylaştırır.\n",
    "\n",
    "## 8) Eğitim grafikleri kodu ne yapıyor?\n",
    "\n",
    "* `hist = pd.DataFrame(history.history)` ile train/val **loss** ve **AUC** eğrilerini çiziyorsun.\n",
    "* Loss düşerken val\\_loss yükselirse overfit; her ikisi de düşüyorsa iyi; val düzleşirse LR azaltma/erken durdurma devrede.\n",
    "\n",
    "## 9) Test değerlendirme (0.5 eşik)\n",
    "\n",
    "* `y_proba = model.predict(X_test_p).ravel()` → **olasılıklar** (0–1).\n",
    "* `y_pred = (y_proba >= 0.5).astype(int)` → **etiket** (0/1). 0.5 **varsayılan** bir eşiktir; problemi/öncelikleri göre ayarlanabilir.\n",
    "* Metrikler:\n",
    "\n",
    "  * **AUC** (ROC AUC): eşikten bağımsız ayrım gücü.\n",
    "  * **Accuracy/Precision/Recall/F1**: eşik **seçimine bağlı** sonuçlar.\n",
    "* `classification_report` ve `confusion_matrix` sınıf bazlı performansı gösterir.\n",
    "\n",
    "## 10) ROC ve PR eğrileri\n",
    "\n",
    "* **ROC**: FPR–TPR eğrisi, **AUC** ile özetlenir. Sınıf dağılımı dengeliyken daha açıklayıcı.\n",
    "* **PR**: Precision–Recall eğrisi, **AP (Average Precision)** ile özetlenir. Pozitif sınıf **nadir** olduğunda daha bilgilendiricidir.\n",
    "\n",
    "## 11) Eşik (threshold) optimizasyonu\n",
    "\n",
    "* 0.1–0.9 arasında eşikleri tarayıp **F1’i maksimize** eden eşiği arıyorsun.\n",
    "* F1, precision/recall dengesini sağlar. Maliyet fonksiyonuna göre **başka bir metrik** de seçilebilirdi (örneğin recall öncelikli ise recall\\@threshold’u maksimize etmek).\n",
    "\n",
    "---\n",
    "\n",
    "# Uygulama: Örnek Verilerle Tahmin, Model Kaydet/Yükle ve Devam Eğitimi\n",
    "\n",
    "Aşağıdaki bloklar, **senin kurduğun pipeline ile birebir uyumlu**.\n",
    "Not: Verdiğin “5 satır” örnekte **4 satır** var; o şekilde kullandım.\n",
    "\n",
    "## A) Örnek veriden tahmin alma\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Örnek veriyi DataFrame'e çevir\n",
    "sample_csv = \"\"\"Age,Height_cm,Weight_kg,Position,Training_Hours_Per_Week,Matches_Played_Past_Season,Previous_Injury_Count,Knee_Strength_Score,Hamstring_Flexibility,Reaction_Time_ms,Balance_Test_Score,Sprint_Speed_10m_s,Agility_Score,Sleep_Hours_Per_Night,Stress_Level_Score,Nutrition_Quality_Score,Warmup_Routine_Adherence,Injury_Next_Season,BMI\n",
    "22,173,64,Midfielder,11.575308026077138,36,1,77.46027901003853,79.11573817783952,284.48785262294393,91.21247628562678,5.874629899095096,77.59970508640522,8.238293030872281,46.61641520851219,81.47220606471353,1,0,21.383941996057334\n",
    "18,170,67,Midfielder,12.275869450190173,37,2,72.6344422606009,82.54168794778663,250.57924940551655,87.29407824138767,5.796269352614358,94.4189869374685,8.983737193266625,49.36803674229354,81.05667689014356,1,0,23.18339100346021\n",
    "22,186,75,Forward,12.254895659208355,12,2,77.0644897841478,75.94363053556577,269.11991825792603,83.4406884700333,5.731208756193826,70.17917629629453,7.229192653844745,43.13280804133722,64.87745688032112,0,1,21.678806798473808\n",
    "20,172,62,Defender,9.00667756614741,11,1,82.81023161351551,73.8783244426281,226.3764118453216,87.59189360708152,6.220212239929888,83.47382412454475,7.681028808856083,51.528529395795616,89.82474393936592,1,0,20.9572742022715\n",
    "\"\"\"\n",
    "from io import StringIO\n",
    "sample_df = pd.read_csv(StringIO(sample_csv))\n",
    "\n",
    "# 2) Hedefi ayır (varsa). Tahmin alırken hedefe ihtiyacımız yok.\n",
    "X_new = sample_df.drop(columns=[\"Injury_Next_Season\"])\n",
    "\n",
    "# 3) Eğitimde fit ettiğin PREPROCESSOR'u kullanarak dönüştür\n",
    "# preprocess ve model zaten bellekteyse direkt kullan:\n",
    "X_new_p = preprocess.transform(X_new)\n",
    "\n",
    "# 4) Olasılık ve etiket tahminleri\n",
    "y_new_proba = model.predict(X_new_p).ravel()\n",
    "y_new_pred  = (y_new_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"Olasılıklar:\", y_new_proba.round(3))\n",
    "print(\"Tahminler  :\", y_new_pred.tolist())\n",
    "```\n",
    "\n",
    "> Not: Pipeline’daki `OneHotEncoder(handle_unknown=\"ignore\")` sayesinde **yeni bir Position** gelse bile hata almazsın (sıfır vektörü verir).\n",
    "\n",
    "---\n",
    "\n",
    "## B) Modeli ve ön-işlemeyi kaydetmek / yüklemek\n",
    "\n",
    "### Kaydetme\n",
    "\n",
    "```python\n",
    "# 1) Keras modeli kaydet\n",
    "model.save(\"injury_ann.keras\")  # veya 'injury_ann.h5'\n",
    "\n",
    "# 2) Sklearn preprocessörü kaydet\n",
    "import joblib\n",
    "joblib.dump(preprocess, \"preprocess.joblib\")\n",
    "\n",
    "# (İsteğe bağlı) Eşik ve versiyon notları:\n",
    "best_threshold_to_save = 0.5  # istersen hesapladığın t_star'ı kaydet\n",
    "meta = {\"threshold\": best_threshold_to_save, \"version\": \"v1.0\"}\n",
    "joblib.dump(meta, \"meta.joblib\")\n",
    "```\n",
    "\n",
    "### Yükleme ve kullanma\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Yükle\n",
    "loaded_model = keras.models.load_model(\"injury_ann.keras\")\n",
    "loaded_preprocess = joblib.load(\"preprocess.joblib\")\n",
    "meta = joblib.load(\"meta.joblib\")  # threshold vb.\n",
    "\n",
    "# 2) Yeni veriyi oku ve dönüştür\n",
    "X_infer = X_new  # örnek olarak yukarıdaki sample'dan\n",
    "X_infer_p = loaded_preprocess.transform(X_infer)\n",
    "\n",
    "# 3) Tahmin\n",
    "proba = loaded_model.predict(X_infer_p).ravel()\n",
    "pred  = (proba >= meta[\"threshold\"]).astype(int)\n",
    "print(\"proba:\", proba)\n",
    "print(\"pred :\", pred)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## C) Yeni veriyle modeli “beslemek” (devam eğitimi)\n",
    "\n",
    "Burada iki ayrı katman var:\n",
    "\n",
    "1. **Ön-işleme** (imputer, scaler, OHE):\n",
    "\n",
    "* **Seçenek-1 (Stabil üretim)**: **Eski preprocessörü dondur** ve **yeni veriyi de aynı dönüşümle** geçir.\n",
    "\n",
    "  * Avantaj: Üretimde istikrar, kolay yönetim.\n",
    "  * Dezavantaj: Yeni **kategoriler** (yeni `Position`) OHE’de **öğrenilmez**, “ignore” edilir (vektörleri sıfır kalır).\n",
    "* **Seçenek-2 (Güncelleme)**: **Eski veri + yeni veriyi birleştirip** preprocessörü **yeniden fit** et.\n",
    "\n",
    "  * Avantaj: Yeni kategoriler/features dağılımları tanınır.\n",
    "  * Dezavantaj: Eski modelle birebir aynı giriş boyutu olmayabilir (OHE boyutu değişebilir); **modeli de yeniden eğitmek** gerekir.\n",
    "\n",
    "2. **Model**:\n",
    "\n",
    "* **Devam eğitimi (fine-tune)**: Aynı mimari ile, **yüklediğin modeli** yeni verinin **dönüştürülmüş** (preprocess edilmiş) versiyonu üstünde **küçük LR** ile eğitmeye devam edebilirsin.\n",
    "\n",
    "### Devam eğitimi – Kod\n",
    "\n",
    "```python\n",
    "# Varsayım: \"loaded_model\" ve \"loaded_preprocess\" yüklendi.\n",
    "# Yeni veriniz (CSV) var:\n",
    "new_df = pd.read_csv(\"football_injury_new.csv\")\n",
    "\n",
    "# Hedefi ayır\n",
    "y_new = new_df[\"Injury_Next_Season\"].values\n",
    "X_new = new_df.drop(columns=[\"Injury_Next_Season\"])\n",
    "\n",
    "# Seçenek-1: preprocessörü dondurup kullan\n",
    "X_new_p = loaded_preprocess.transform(X_new)\n",
    "\n",
    "# Devam eğitimi için düşük öğrenme oranı\n",
    "from tensorflow.keras import optimizers\n",
    "loaded_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=5e-4),  # başlangıca göre daha küçük LR\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\"),\n",
    "             keras.metrics.Precision(name=\"precision\"),\n",
    "             keras.metrics.Recall(name=\"recall\")]\n",
    ")\n",
    "\n",
    "# class_weight istersen yine hesapla (yeni eğitim setine göre)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "classes = np.unique(y_new)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_new)\n",
    "cw_dict = {int(c): w for c, w in zip(classes, cw)}\n",
    "\n",
    "early2 = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_auc\", mode=\"max\", patience=5, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "# küçük bir validasyon ayır\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xn_tr, Xn_va, yn_tr, yn_va = train_test_split(X_new_p, y_new, test_size=0.2, stratify=y_new, random_state=1)\n",
    "\n",
    "hist2 = loaded_model.fit(\n",
    "    Xn_tr, yn_tr,\n",
    "    validation_data=(Xn_va, yn_va),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=cw_dict,\n",
    "    callbacks=[early2],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Güncellenmiş modeli tekrar kaydet\n",
    "loaded_model.save(\"injury_ann_finetuned.keras\")\n",
    "```\n",
    "\n",
    "> **Önemli notlar**\n",
    ">\n",
    "> * **Seçenek-2** (preprocessörü yeniden fit etmek) yapacaksan, **X\\_train/X\\_valid/X\\_test** ile **X\\_new**’i birleştir, **yeni bir `preprocess.fit`** yap, **ağ giriş boyutu** (OHE nedeniyle) değişebileceği için **modeli sıfırdan** (veya ilk layer’i adapte ederek) yeniden eğit.\n",
    "> * Üretim hattında genelde **Seçenek-1** tercih edilir; belirli periyotlarda “versiyon yükseltmesi” ile Seçenek-2 planlı yapılır.\n",
    "\n",
    "---\n",
    "\n",
    "## D) (İsteğe bağlı) En iyi eşiği kaydedip yüklemek\n",
    "\n",
    "Tahmin sonrası **karar eşiğini** (ör. `t_star`) kaydedip üretimde kullan:\n",
    "\n",
    "```python\n",
    "# Hesaplamıştın:\n",
    "# t_star, s_star = best_threshold(y_test, y_proba, metric=\"f1\")\n",
    "\n",
    "import joblib\n",
    "joblib.dump({\"threshold\": float(t_star)}, \"decision_threshold.joblib\")\n",
    "\n",
    "# Yükleyip kullanma:\n",
    "th = joblib.load(\"decision_threshold.joblib\")[\"threshold\"]\n",
    "pred = (proba >= th).astype(int)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Kısa Özet (sorularının çekirdeği)\n",
    "\n",
    "* **`y.values`**: y’yi array yapmak doğal; **X’i DataFrame** tutmamız **kolon isimleriyle** çalışan `ColumnTransformer` için şart.\n",
    "* **3’lü split**: Train/Valid/Test ayrımı için iki aşamalı bölme.\n",
    "* **`fit_transform(train)` / `transform(valid/test)`**: sızıntıyı önlemek ve gerçekçi değerlendirme için.\n",
    "* **`feature_dim`**: OHE sonrası gerçek giriş boyutunu almak için.\n",
    "* **`class_weight`**: Dengeyi korumak, küçük dengesizlikleri telafi etmek.\n",
    "* **Sequential**: Zaten Sequential kullandın; Functional API, çoklu giriş/karmaşık mimaride gerekli olur.\n",
    "* **EarlyStopping & ReduceLROnPlateau**: Overfit’i ve gereksiz epoch’ları engeller, LR’ı dinamik küçültür.\n",
    "* **`history`**: Eğitim metriklerinin zaman içi kaydı; grafikle teşhis yaparsın.\n",
    "* **ROC/PR & threshold**: Eşik-agnostik (AUC/AP) analiz + uygulama eşiğini probleme göre optimize etme.\n",
    "\n",
    "Takıldığın başka nokta varsa veya **embedding’li pozisyon temsili** versiyonunu da (ANN içinde) görmek istersen, onu da tam kodla veririm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b230e-9de1-489f-8db8-23ab17cc0e26",
   "metadata": {},
   "source": [
    "**“Eşik değeri (threshold)”**:\n",
    "Modelin çıktısı **olasılıktır** (ör. `P(injury=1)` = 0.73). Bu olasılığı **etikete** (0/1) çevirmek için bir **karar eşiği** seçersin.\n",
    "\n",
    "* Eğer `p >= eşik` ⇒ **1 (İnjury)**, değilse **0**.\n",
    "* **Varsayılan** genellikle **0.50**’dir, ama bu **sabit** olmak zorunda değil.\n",
    "\n",
    "Niye önemli?\n",
    "\n",
    "* **Precision–Recall** dengesini ve **F1**, **hata maliyetlerini** doğrudan etkiler. Örn. “**kaçırmayalım**” (high recall) istiyorsan **eşiği düşürürsün**; “**yanlış alarm vermeyelim**” (high precision) istiyorsan **eşiği yükseltirsin**.\n",
    "* Eğitim sırasında **model “eşik” öğrenmez**; yalnızca olasılık üretir. **Eşik**, senin **iş kuralın**dır (post-processing karar kuralı).\n",
    "\n",
    "Niye kaydediyoruz?\n",
    "\n",
    "* **Tutarlılık**: Üretimde/raporlarda **aynı karar noktasını** kullanmak için (bugün 0.30, yarın 0.50 olmasın).\n",
    "* **İzlenebilirlik**: “Bu model v1.2, **eşik=0.30** ile çalışıyor” diyebilmek.\n",
    "* **Esneklik**: Eşiği **modeli yeniden eğitmeden** değiştirebilirsin (ör. sahada çok alarm geliyorsa 0.30→0.40).\n",
    "\n",
    "Eşiği nasıl seçerim?\n",
    "\n",
    "* **F1’i maksimize et** (senin koddaki gibi): hem precision hem recall dengesi.\n",
    "* **Youden’s J / ROC** (TPR–FPR farkını maksimize et): sınıf maliyetleri benzerse iyi başlangıç.\n",
    "* **İş kuralı hedefi**: “Recall ≥ 0.95 olsun” ya da “Precision ≥ 0.90 olsun” gibi kısıtlarla PR eğrisinden eşik seç.\n",
    "* **Maliyet tabanlı**: FP, FN maliyetlerini ver; beklenen maliyeti minimize eden eşiği seç.\n",
    "* **Kalibrasyon**: Olasılıklar iyi kalibre değilse (Platt/Isotonic), önce kalibre et, sonra eşiği seç.\n",
    "\n",
    "Kısa örnekler (senin yapına uygun):\n",
    "\n",
    "```python\n",
    "# 1) F1’i maksimize eden eşik\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "def best_threshold_f1(y_true, y_prob):\n",
    "    ts = np.linspace(0.01, 0.99, 99)\n",
    "    best_t, best_s = 0.5, -1\n",
    "    for t in ts:\n",
    "        s = f1_score(y_true, (y_prob >= t).astype(int), zero_division=0)\n",
    "        if s > best_s: best_s, best_t = s, t\n",
    "    return best_t, best_s\n",
    "\n",
    "t_star, s_star = best_threshold_f1(y_valid, model.predict(X_valid_p).ravel())\n",
    "print(\"F1-opt eşik:\", round(t_star, 2), \"F1:\", round(s_star, 3))\n",
    "```\n",
    "\n",
    "```python\n",
    "# 2) \"Precision ≥ 0.90\" şartıyla en yüksek recall'u veren eşik\n",
    "prec, rec, thr = precision_recall_curve(y_valid, model.predict(X_valid_p).ravel())\n",
    "target_prec = 0.90\n",
    "candidates = np.where(prec[:-1] >= target_prec)[0]  # son eleman thr ile hizalı değil\n",
    "i = candidates[np.argmax(rec[candidates])] if len(candidates) else np.argmax(prec[:-1])\n",
    "t_star = thr[i]\n",
    "print(\"Precision≥0.90 için eşik:\", round(t_star, 2), \"Recall:\", round(rec[i], 3))\n",
    "```\n",
    "\n",
    "Kaydet/Yükle (eşik bir model parametresi değildir; **ayrı meta** olarak tutulur):\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# kaydetme\n",
    "joblib.dump({\"threshold\": float(t_star), \"version\": \"v1.0\"}, \"decision_threshold.joblib\")\n",
    "\n",
    "# yükleme ve kullanma\n",
    "meta = joblib.load(\"decision_threshold.joblib\")\n",
    "th = meta[\"threshold\"]\n",
    "y_pred = (model.predict(X_test_p).ravel() >= th).astype(int)\n",
    "```\n",
    "\n",
    "Özet:\n",
    "\n",
    "* **Eşik**, olasılık → sınıf dönüşümü için **karar kuralın**; **modelin parçası değildir**.\n",
    "* **Seçimi**, **hedef metriklerine** ve **iş maliyetlerine** bağlıdır.\n",
    "* **Kaydetmek**, üretimde **istikrar** ve **tekrar edilebilirlik** sağlar; gerektiğinde **tek başına ayarlanabilir**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8837d6-7cd7-4f48-9e15-d7a2d4efdac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
