{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0192b9e6-c7c5-4d0f-80a6-c1201ff7773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users=943 | Items=1682 | Train obs=80000 | Test obs=20000\n",
      "Epoch 1/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 9.7401 \n",
      "Epoch 1: val_loss improved from inf to 4.24531, saving model to ml100k_ae_best.keras\n",
      "8/8 [==============================] - 2s 96ms/step - loss: 9.4767 - val_loss: 4.2453 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 3.6015\n",
      "Epoch 2: val_loss improved from 4.24531 to 2.34077, saving model to ml100k_ae_best.keras\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.5929 - val_loss: 2.3408 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 2.0264\n",
      "Epoch 3: val_loss improved from 2.34077 to 1.53806, saving model to ml100k_ae_best.keras\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.0065 - val_loss: 1.5381 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.6344\n",
      "Epoch 4: val_loss did not improve from 1.53806\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 1.6415 - val_loss: 1.7214 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.5084\n",
      "Epoch 5: val_loss improved from 1.53806 to 1.25604, saving model to ml100k_ae_best.keras\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.4984 - val_loss: 1.2560 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.3330\n",
      "Epoch 6: val_loss improved from 1.25604 to 1.17404, saving model to ml100k_ae_best.keras\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.3487 - val_loss: 1.1740 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.2278\n",
      "Epoch 7: val_loss did not improve from 1.17404\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.2322 - val_loss: 1.2109 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.1754\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.17404\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.1791 - val_loss: 1.2946 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.1947\n",
      "Epoch 9: val_loss did not improve from 1.17404\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.1921 - val_loss: 1.2263 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.1552\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.17404\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 1.1573 - val_loss: 1.2049 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.1208\n",
      "Epoch 11: val_loss improved from 1.17404 to 1.04827, saving model to ml100k_ae_best.keras\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.1258 - val_loss: 1.0483 - lr: 2.5000e-04\n",
      "Epoch 12/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.1089\n",
      "Epoch 12: val_loss did not improve from 1.04827\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.1118 - val_loss: 1.2029 - lr: 2.5000e-04\n",
      "Epoch 13/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0987\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.04827\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.1004 - val_loss: 1.0738 - lr: 2.5000e-04\n",
      "Epoch 14/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0893\n",
      "Epoch 14: val_loss did not improve from 1.04827\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 1.0869 - val_loss: 1.1421 - lr: 1.2500e-04\n",
      "Epoch 15/50\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0815\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.04827\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0821 - val_loss: 1.0770 - lr: 1.2500e-04\n",
      "Epoch 16/50\n",
      "Restoring model weights from the end of the best epoch: 11.19\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.04827\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 1.0715 - val_loss: 1.0995 - lr: 6.2500e-05\n",
      "Epoch 16: early stopping\n",
      "[TEST] RMSE: 1.0041\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
    "DATA_DIR = \"ml-100k\"\n",
    "\n",
    "def load_movielens_100k(data_dir):\n",
    "    base_path = os.path.join(data_dir, \"u1.base\")\n",
    "    test_path = os.path.join(data_dir, \"u1.test\")\n",
    "    data_path = os.path.join(data_dir, \"u.data\")\n",
    "\n",
    "    if os.path.exists(base_path) and os.path.exists(test_path):\n",
    "        train_df = pd.read_csv(base_path, sep=\"\\t\", header=None,\n",
    "                               names=[\"user\",\"item\",\"rating\",\"ts\"])\n",
    "        test_df  = pd.read_csv(test_path,  sep=\"\\t\", header=None,\n",
    "                               names=[\"user\",\"item\",\"rating\",\"ts\"])\n",
    "    elif os.path.exists(data_path):\n",
    "        df = pd.read_csv(data_path, sep=\"\\t\", header=None,\n",
    "                         names=[\"user\",\"item\",\"rating\",\"ts\"])\n",
    "        train_rows, test_rows = [], []\n",
    "        for uid, grp in df.groupby(\"user\"):\n",
    "            grp = grp.sample(frac=1.0, random_state=SEED)\n",
    "            n_test = max(1, int(0.2*len(grp)))\n",
    "            test_rows.append(grp.iloc[:n_test])\n",
    "            train_rows.append(grp.iloc[n_test:])\n",
    "        train_df = pd.concat(train_rows).reset_index(drop=True)\n",
    "        test_df  = pd.concat(test_rows).reset_index(drop=True)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"MovieLens 100k dosyaları bulunamadı.\")\n",
    "\n",
    "    # tipleri garantiye al\n",
    "    for col in [\"user\",\"item\",\"rating\"]:\n",
    "        train_df[col] = pd.to_numeric(train_df[col], errors=\"raise\")\n",
    "        test_df[col]  = pd.to_numeric(test_df[col],  errors=\"raise\")\n",
    "\n",
    "    n_users = int(max(train_df[\"user\"].max(), test_df[\"user\"].max()))\n",
    "    n_items = int(max(train_df[\"item\"].max(), test_df[\"item\"].max()))\n",
    "\n",
    "    # --- Hızlı ve güvenli kurulum: pivot -> değerler ---\n",
    "    def build_matrix_fast(df, n_users, n_items):\n",
    "        # kullanıcı ve item id'lerini 0-index'e indir\n",
    "        piv = df.copy()\n",
    "        piv[\"user0\"] = piv[\"user\"].astype(int) - 1\n",
    "        piv[\"item0\"] = piv[\"item\"].astype(int) - 1\n",
    "        mat = np.zeros((n_users, n_items), dtype=np.float32)\n",
    "        mat[piv[\"user0\"].to_numpy(), piv[\"item0\"].to_numpy()] = piv[\"rating\"].to_numpy(dtype=np.float32)\n",
    "        return mat\n",
    "\n",
    "    train_mat = build_matrix_fast(train_df, n_users, n_items)\n",
    "    test_mat  = build_matrix_fast(test_df,  n_users, n_items)\n",
    "\n",
    "    train_mask = (train_mat > 0).astype(np.float32)\n",
    "    test_mask  = (test_mat  > 0).astype(np.float32)\n",
    "\n",
    "    print(f\"Users={n_users} | Items={n_items} | \"\n",
    "          f\"Train obs={int(train_mask.sum())} | Test obs={int(test_mask.sum())}\")\n",
    "    return train_mat, test_mat, train_mask, test_mask\n",
    "\n",
    "# BURADAN SONRASI önceki AutoEncoder kodunla aynı:\n",
    "train_mat, test_mat, train_mask, test_mask = load_movielens_100k(DATA_DIR)\n",
    "\n",
    "@tf.function\n",
    "def masked_mse(y_true, y_pred):\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0.0), tf.float32)\n",
    "    diff = (y_true - y_pred) * mask\n",
    "    return tf.reduce_sum(tf.square(diff)) / (tf.reduce_sum(mask) + 1e-8)\n",
    "\n",
    "def build_ae(n_items, l2=1e-4, dropout=0.2, bottleneck=128):\n",
    "    inp = layers.Input(shape=(n_items,))\n",
    "    x = layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(l2))(inp)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    z = layers.Dense(bottleneck, activation=\"relu\", kernel_regularizer=regularizers.l2(l2), name=\"bottleneck\")(x)\n",
    "    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(l2))(z)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    out = layers.Dense(n_items, activation=None)(x)\n",
    "    return models.Model(inp, out)\n",
    "\n",
    "n_users, n_items = train_mat.shape\n",
    "ae = build_ae(n_items)\n",
    "ae.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=masked_mse)\n",
    "\n",
    "es  = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1, min_lr=1e-5)\n",
    "mc  = callbacks.ModelCheckpoint(\"ml100k_ae_best.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1)\n",
    "\n",
    "history = ae.fit(\n",
    "    train_mat, train_mat,\n",
    "    validation_data=(train_mat, train_mat),\n",
    "    epochs=50, batch_size=128, shuffle=True,\n",
    "    callbacks=[es, rlr, mc], verbose=1\n",
    ")\n",
    "\n",
    "best = tf.keras.models.load_model(\"ml100k_ae_best.keras\", custom_objects={\"masked_mse\": masked_mse})\n",
    "pred = np.clip(best.predict(train_mat, verbose=0), 1.0, 5.0)\n",
    "\n",
    "# Basit RMSE (yalnızca test maskeli yerlerde)\n",
    "sqerr = ((pred - test_mat) ** 2) * (test_mat > 0)\n",
    "rmse = float(np.sqrt(sqerr.sum() / (test_mat > 0).sum()))\n",
    "print(f\"[TEST] RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a09a0ea-d5ba-446b-8793-39b338b46ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-K değerlendirme (relevant>=4):\n",
      "@5: Precision=0.0026 | Recall=0.0003 | users=456\n",
      "@10: Precision=0.0055 | Recall=0.0015 | users=456\n",
      "@20: Precision=0.0202 | Recall=0.0117 | users=456\n",
      "\n",
      "Kullanıcı #1 için öneriler (itemID:score):\n",
      "1367: 5.000\n",
      "1599: 4.863\n",
      "1467: 4.822\n",
      "1449: 4.668\n",
      "1452: 4.598\n",
      "1368: 4.597\n",
      "408: 4.570\n",
      "913: 4.570\n",
      "1064: 4.549\n",
      "1500: 4.535\n",
      "\n",
      "Model kaydedildi: ml100k_ae_best.keras\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 6) Precision / Recall @K (kullanıcı-bazlı, train'de görülmeyenlere öner)\n",
    "# ---------------------------------------------------\n",
    "def evaluate_topk(pred_scores, train_mat, test_mat, K_list=(5, 10, 20), rel_thresh=4.0):\n",
    "    \"\"\"\n",
    "    pred_scores: (n_users, n_items) tahmin puanları\n",
    "    train_mat:   eğitimde görülen puanlar (görülen item'ları hariç tutacağız)\n",
    "    test_mat:    testteki gerçek puanlar\n",
    "    rel_thresh:  testte \"ilgili\" kabul eşiği (>=4.0 => ilgili)\n",
    "    \"\"\"\n",
    "    train_seen = (train_mat > 0)\n",
    "    test_rel   = (test_mat >= rel_thresh)\n",
    "\n",
    "    results = {}\n",
    "    for K in K_list:\n",
    "        tot_prec, tot_rec, n_users_eval = 0.0, 0.0, 0\n",
    "        for u in range(pred_scores.shape[0]):\n",
    "            # bu kullanıcının testte ilgili sayısı:\n",
    "            n_rel = int(test_rel[u].sum())\n",
    "            if n_rel == 0:\n",
    "                continue  # kullanıcıda ölçülecek ilgili yoksa atla\n",
    "\n",
    "            scores = pred_scores[u].copy()\n",
    "            # eğitimde görülen item'ları öneriden çıkar\n",
    "            scores[train_seen[u]] = -1e9\n",
    "\n",
    "            topk_idx = np.argpartition(scores, -K)[-K:]\n",
    "            topk_idx = topk_idx[np.argsort(scores[topk_idx])[::-1]]  # skor sırasına getir\n",
    "\n",
    "            hits = test_rel[u, topk_idx].sum()\n",
    "            prec = float(hits) / K\n",
    "            rec  = float(hits) / n_rel\n",
    "\n",
    "            tot_prec += prec\n",
    "            tot_rec  += rec\n",
    "            n_users_eval += 1\n",
    "\n",
    "        results[K] = {\n",
    "            \"precision\": tot_prec / max(1, n_users_eval),\n",
    "            \"recall\":    tot_rec  / max(1, n_users_eval),\n",
    "            \"n_users\":   n_users_eval\n",
    "        }\n",
    "    return results\n",
    "\n",
    "metrics = evaluate_topk(pred, train_mat, test_mat, K_list=(5,10,20), rel_thresh=4.0)\n",
    "print(\"\\nTop-K değerlendirme (relevant>=4):\")\n",
    "for K, m in metrics.items():\n",
    "    print(f\"@{K}: Precision={m['precision']:.4f} | Recall={m['recall']:.4f} | users={m['n_users']}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7) Tek kullanıcı için öneri örneği\n",
    "# ---------------------------------------------------\n",
    "def recommend_for_user(user_id_1based, pred_scores, train_mat, top_k=10):\n",
    "    u = user_id_1based - 1\n",
    "    scores = pred_scores[u].copy()\n",
    "    scores[train_mat[u] > 0] = -1e9  # zaten izlediklerini ele\n",
    "    topk_idx = np.argpartition(scores, -top_k)[-top_k:]\n",
    "    topk_idx = topk_idx[np.argsort(scores[topk_idx])[::-1]]\n",
    "    # 1-based item ID’leri döndürelim\n",
    "    items_1based = (topk_idx + 1).tolist()\n",
    "    return items_1based, scores[topk_idx].tolist()\n",
    "\n",
    "# Örnek: kullanıcı 1 için 10 öneri\n",
    "items, scores = recommend_for_user(1, pred, train_mat, top_k=10)\n",
    "print(\"\\nKullanıcı #1 için öneriler (itemID:score):\")\n",
    "for iid, sc in zip(items, scores):\n",
    "    print(f\"{iid}: {sc:.3f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 8) Modeli kaydet (ileride tekrar yüklemek için)\n",
    "# ---------------------------------------------------\n",
    "best.save(\"ml100k_ae_best.keras\")\n",
    "print(\"\\nModel kaydedildi: ml100k_ae_best.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31276d85-b82c-4222-8fd8-f2fd3cac3619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation, Children, Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>Action, Adventure, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>Action, Comedy, Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId              title                       genres\n",
       "0        1   Toy Story (1995)  Animation, Children, Comedy\n",
       "1        2   GoldenEye (1995)  Action, Adventure, Thriller\n",
       "2        3  Four Rooms (1995)                     Thriller\n",
       "3        4  Get Shorty (1995)        Action, Comedy, Drama\n",
       "4        5     Copycat (1995)       Crime, Drama, Thriller"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_movie_meta(data_dir=DATA_DIR):\n",
    "    uitem = os.path.join(data_dir, \"u.item\")\n",
    "    mdat  = os.path.join(\"ml-1m\", \"movies.dat\")  # varsa 1M\n",
    "    if os.path.exists(uitem):\n",
    "        # MovieLens 100k\n",
    "        genre_cols = [\"unknown\",\"Action\",\"Adventure\",\"Animation\",\"Children\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\n",
    "                      \"Fantasy\",\"Film-Noir\",\"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"War\",\"Western\"]\n",
    "        cols = [\"movieId\",\"title\",\"release_date\",\"video_release_date\",\"imdb_url\"] + genre_cols\n",
    "        meta = pd.read_csv(uitem, sep=\"|\", header=None, names=cols, encoding=\"latin-1\")\n",
    "        # Tür listesini tek kolonda topla (True olanları birleştir)\n",
    "        genres = []\n",
    "        gmat = meta[genre_cols].astype(bool)\n",
    "        for i in range(len(meta)):\n",
    "            glist = [g for g,flag in zip(genre_cols, gmat.iloc[i].tolist()) if flag]\n",
    "            genres.append(\", \".join(glist) if glist else \"Unknown\")\n",
    "        meta = meta[[\"movieId\",\"title\"]].assign(genres=genres)\n",
    "        return meta\n",
    "    elif os.path.exists(mdat):\n",
    "        # MovieLens 1M (opsiyonel)\n",
    "        meta = pd.read_csv(mdat, sep=\"::\", engine=\"python\", header=None,\n",
    "                           names=[\"movieId\",\"title\",\"genres\"], encoding=\"latin-1\")\n",
    "        return meta[[\"movieId\",\"title\",\"genres\"]]\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Ne ml-100k/u.item ne de ml-1m/movies.dat bulunamadı.\")\n",
    "\n",
    "meta = load_movie_meta(DATA_DIR)\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62090c02-eb23-4fe5-b19e-1074c781a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " movieId                                title                      genres    score\n",
      "    1367                         Faust (1994)                   Animation 4.656963\n",
      "    1599        Someone Else's America (1995)                       Drama 4.608151\n",
      "    1467 Saint of Fort Washington, The (1993)                       Drama 4.593640\n",
      "    1449               Pather Panchali (1955)                       Drama 4.366854\n",
      "    1645              Butcher Boy, The (1998)                       Drama 4.275976\n",
      "    1235          Big Bang Theory, The (1994)                       Crime 4.148536\n",
      "     913 Love and Death on Long Island (1997)                      Comedy 4.140354\n",
      "    1064                     Crossfire (1947)            Crime, Film-Noir 4.102300\n",
      "     408                Close Shave, A (1995) Animation, Comedy, Thriller 4.101787\n",
      "    1500            Santa with Muscles (1996)                      Comedy 4.097122\n"
     ]
    }
   ],
   "source": [
    "def recommend_for_user(user_id, score_matrix, train_mat, meta_df, topk=10, min_score=1.0, clip_to_1_5=True):\n",
    "    \"\"\"\n",
    "    user_id: 1-index (MovieLens kullanıcı id'si gibi)\n",
    "    score_matrix: model çıktı skoru (ör: best.predict(train_mat)) shape=(n_users, n_items)\n",
    "    train_mat: orijinal eğitim matrisi (izlenen/puanlananları elemek için)\n",
    "    meta_df: load_movie_meta çıktısı (movieId=1..n_items)\n",
    "    \"\"\"\n",
    "    uidx = user_id - 1  # 0-index'e indir\n",
    "    scores = score_matrix[uidx].copy()\n",
    "\n",
    "    # İstersek 1..5’e kırp\n",
    "    if clip_to_1_5:\n",
    "        scores = np.clip(scores, 1.0, 5.0)\n",
    "\n",
    "    # Kullanıcının zaten puanladıklarını çıkar\n",
    "    already_rated = train_mat[uidx] > 0\n",
    "    scores[already_rated] = -np.inf\n",
    "\n",
    "    # En yüksek skorlu top-k indeksleri al\n",
    "    top_idx = np.argpartition(-scores, range(topk))[:topk]\n",
    "    top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "\n",
    "    # movieId = column_index + 1 (100k setinde kolonlar 1..1682)\n",
    "    rec_df = pd.DataFrame({\n",
    "        \"movieId\": top_idx + 1,\n",
    "        \"score\": scores[top_idx]\n",
    "    })\n",
    "    rec_df = rec_df.merge(meta_df, on=\"movieId\", how=\"left\")\n",
    "    # skor eşiği uygula (opsiyonel)\n",
    "    rec_df = rec_df[rec_df[\"score\"] >= min_score]\n",
    "    # kolon sırası ve format\n",
    "    rec_df = rec_df[[\"movieId\",\"title\",\"genres\",\"score\"]]\n",
    "    return rec_df.reset_index(drop=True)\n",
    "\n",
    "# --- ÖRNEK KULLANIM ---\n",
    "# Eğitimden sonra zaten şunları üretmiştin:\n",
    "# best = tf.keras.models.load_model(\"ml100k_ae_best.keras\", custom_objects={\"masked_mse\": masked_mse})\n",
    "# pred = np.clip(best.predict(train_mat, verbose=0), 1.0, 5.0)\n",
    "\n",
    "user_id = 11\n",
    "rec_df = recommend_for_user(user_id, pred, train_mat, meta, topk=10)\n",
    "print(rec_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb9c8b-7837-4971-a91f-1f13277fdfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
